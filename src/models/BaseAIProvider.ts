import axios, { AxiosInstance } from 'axios'
import { Message, AIModel } from '../types/index.js'
import { withRetry, RetryConfig } from '../utils/retry.js'
import { config } from '../config/index.js'

export abstract class BaseAIProvider {
  protected client: AxiosInstance
  protected model: AIModel
  protected retryConfig: RetryConfig

  constructor(apiKey: string, baseUrl: string, model: AIModel) {
    this.model = model
    this.retryConfig = config.retry

    this.client = axios.create({
      baseURL: baseUrl,
      headers: {
        'Content-Type': 'application/json',
      },
      timeout: config.aiRequest.timeout,
    })

    this.setupAuth(apiKey)
  }

  protected abstract setupAuth(apiKey: string): void
  protected abstract formatMessages(messages: Message[]): any[]
  protected abstract parseResponse(response: any): string
  protected abstract parseStreamResponse(chunk: string): string | null

  async generateResponse(messages: Message[], systemPrompt?: string): Promise<string> {
    const context = `${this.model.provider}-${this.model.name}`

    return await withRetry(
      async () => {
        // ÁßªÈô§Âú®Ê≠§Â§ÑÈ¢ÑÁΩÆ system message ÁöÑÈÄªËæë
        const formattedMessages = this.formatMessages(messages)

        // ÂºÄÂèëÊ®°Âºè‰∏ãËæìÂá∫ËØ∑Ê±ÇÂÜÖÂÆπ
        if (process.env.NODE_ENV === 'development') {
          console.log(`\nüîµ [${this.model.provider}] Request:`, {
            model: this.model.name,
            systemPrompt: systemPrompt, // Log the separate system prompt
            messages: formattedMessages.map((m: any) => ({
              // Use any type for flexibility
              role: m.role,
              content: m.content.substring(0, 500) + (m.content.length > 500 ? '...' : ''),
            })),
          })
        }

        let response: any
        let parsedResponse: string

        try {
          // È¶ñÂÖàÂ∞ùËØïÊµÅÂºèËØ∑Ê±Ç
          if (config.aiRequest.enableStreamFallback && process.env.NODE_ENV === 'development') {
            console.log(`\nüì° [${this.model.provider}] Trying streaming request...`)
          }

          // Â∞Ü systemPrompt ‰º†ÈÄíÁªô makeStreamRequest
          response = await this.makeStreamRequest(formattedMessages, systemPrompt)
          parsedResponse = await this.handleStreamResponse(response)

          if (process.env.NODE_ENV === 'development') {
            console.log(`\n‚úÖ [${this.model.provider}] Streaming successful`)
          }
        } catch (streamError: any) {
          // ÊµÅÂºèÂ§±Ë¥•ÔºåÂõûÈÄÄÂà∞ÈùûÊµÅÂºèÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
          if (config.aiRequest.enableStreamFallback) {
            if (process.env.NODE_ENV === 'development') {
              console.log(
                `\n‚ö†Ô∏è  [${this.model.provider}] Streaming failed, falling back to non-streaming:`,
                streamError.message
              )
            }

            // Â∞Ü systemPrompt ‰º†ÈÄíÁªô makeRequest
            response = await this.makeRequest(formattedMessages, systemPrompt)
            parsedResponse = this.parseResponse(response.data)

            if (process.env.NODE_ENV === 'development') {
              console.log(`\n‚úÖ [${this.model.provider}] Non-streaming successful`)
            }
          } else {
            throw streamError
          }
        }

        // ÂºÄÂèëÊ®°Âºè‰∏ãËæìÂá∫ÂõûÂ§çÂÜÖÂÆπ
        if (process.env.NODE_ENV === 'development') {
          console.log(`\nüü¢ [${this.model.provider}] Response:`, {
            content: parsedResponse.substring(0, 500) + (parsedResponse.length > 500 ? '...' : ''),
            fullLength: parsedResponse.length,
          })
        }

        // È™åËØÅÂìçÂ∫î‰∏ç‰∏∫Á©∫
        if (!parsedResponse || parsedResponse.trim().length === 0) {
          throw new Error(`Empty response from ${this.model.provider}`)
        }

        return parsedResponse
      },
      this.retryConfig,
      context
    )
  }

  private async handleStreamResponse(response: any): Promise<string> {
    return new Promise(async (resolve, reject) => {
      let fullContent = ''

      try {
        // Ê£ÄÊü•ÊòØÂê¶ÊòØ OpenAI SDK ÁöÑÊµÅÂºèÂìçÂ∫î (AsyncIterable)
        if (response.data && Symbol.asyncIterator in response.data) {
          for await (const chunk of response.data) {
            // OpenAI format
            const openaiContent = chunk.choices?.[0]?.delta?.content
            // Anthropic format
            const anthropicContent = chunk.delta?.text
            // Gemini format
            const geminiContent = chunk.candidates?.[0]?.content?.parts?.[0]?.text || chunk.text?.()

            const content = openaiContent || anthropicContent || geminiContent
            if (content) {
              fullContent += content
            }
          }
          resolve(fullContent)
          return
        }

        // ‰º†ÁªüÁöÑÊµÅÂºèÂìçÂ∫îÂ§ÑÁêÜ (ÈÄÇÁî®‰∫éËá™ÂÆö‰πâÂÆûÁé∞)
        response.data.on('data', (chunk: Buffer) => {
          const lines = chunk
            .toString()
            .split('\n')
            .filter((line) => line.trim())

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6)
              if (data === '[DONE]') {
                resolve(fullContent)
                return
              }

              try {
                const parsed = this.parseStreamResponse(data)
                if (parsed) {
                  fullContent += parsed
                }
              } catch (e) {
                // ÂøΩÁï•Ëß£ÊûêÈîôËØØÔºåÁªßÁª≠Â§ÑÁêÜÂÖ∂‰ªñÂùó
              }
            }
          }
        })

        response.data.on('end', () => {
          resolve(fullContent)
        })

        response.data.on('error', (error: any) => {
          reject(error)
        })
      } catch (error) {
        reject(error)
      }
    })
  }

  protected abstract makeRequest(messages: any[], systemPrompt?: string): Promise<any>
  protected abstract makeStreamRequest(messages: any[], systemPrompt?: string): Promise<any>

  getModel(): AIModel {
    return this.model
  }
}
